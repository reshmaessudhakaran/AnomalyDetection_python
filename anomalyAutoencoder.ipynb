{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME9/TJKqm8VqEAVNAoEBZ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reshmaessudhakaran/AnomalyDetection_python/blob/main/anomalyAutoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ivvnb3LRtbiw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFWOX-AfiI9M",
        "outputId": "50a61c6f-c5fe-41c3-9469-d467e4a52570"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.3)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.26.4)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.13.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.1.4)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uJnkMz0Dum38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf56677b-d950-4e4d-83d0-e919a89bede3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataConsolidation:\n",
        "    def __init__(self, frequency):\n",
        "        self.frequency = frequency\n",
        "\n",
        "    def consolidate(self, data):\n",
        "        try:\n",
        "\n",
        "            data['order_time'] = pd.to_datetime(data['order_time'], format='%d-%m-%Y %H:%M')\n",
        "\n",
        "            if self.frequency == 'H':\n",
        "                data['Date'] = data['order_time'].dt.floor('H')\n",
        "            elif self.frequency == 'D':\n",
        "                data['Date'] = data['order_time'].dt.floor('D')\n",
        "            elif self.frequency == 'W':\n",
        "                data['Date'] = data['order_time'].dt.to_period('W-SUN').apply(lambda r: r.start_time)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported frequency: {self.frequency}\")\n",
        "\n",
        "            data = data.groupby('Date')['total_price'].sum().reset_index()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error consolidating data: {e}\")\n",
        "            return None\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "nMSqtklHwL50"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_week():\n",
        "    return datetime.now().isocalendar()[1]\n",
        "\n",
        "def get_last_retrained_week(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            return int(file.readline().strip())\n",
        "    return None\n",
        "\n",
        "def update_last_retrained_week(file_path, week):\n",
        "    current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(f\"{week}\\nRetrained on: {current_date}\")\n"
      ],
      "metadata": {
        "id": "VqzFDrZw-9kq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_and_preprocess_data(filepath, frequency):\n",
        "    data = pd.read_csv(filepath)\n",
        "    consolidator = DataConsolidation(frequency)\n",
        "    consolidated_data = consolidator.consolidate(data)\n",
        "    return consolidated_data\n",
        "\n",
        "def scale_data(data, scaler=None):\n",
        "    if scaler is None:\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    dataset = data[['total_price']].values\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "    return scaled_data, scaler, dataset\n",
        "\n",
        "def create_train_test_datasets(scaled_data):\n",
        "    x_data = np.expand_dims(scaled_data, axis=-1)\n",
        "    return x_data\n",
        "\n",
        "def build_and_train_autoencoder(x_train, autoencoder_model_path, callbacks=[]):\n",
        "    input_layer_ae = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
        "    encoded_ae = LSTM(32, activation='relu', return_sequences=True)(input_layer_ae)\n",
        "    encoded_ae = LSTM(16, activation='relu', return_sequences=False)(encoded_ae)\n",
        "    decoded_ae = Dense(x_train.shape[1], activation='linear')(encoded_ae)\n",
        "    autoencoder = Model(input_layer_ae, decoded_ae)\n",
        "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "    callbacks = [early_stopping] + callbacks\n",
        "\n",
        "    autoencoder.fit(x_train, x_train, epochs=50, batch_size=32, callbacks=callbacks, verbose=1)\n",
        "    autoencoder.save(autoencoder_model_path.replace('.h5', '.keras'))  # Save as .keras file\n",
        "    return autoencoder\n",
        "\n",
        "def calculate_reconstruction_errors(autoencoder, test_data):\n",
        "    reconstructed_test_data = autoencoder.predict(test_data)\n",
        "    reconstruction_errors = np.mean(np.abs(reconstructed_test_data - test_data), axis=(1, 2))\n",
        "    return reconstruction_errors\n",
        "\n",
        "def calculate_stats(window):\n",
        "    mean = np.mean(window)\n",
        "    std_dev = np.std(window)\n",
        "    return mean, std_dev\n",
        "\n",
        "def calculate_peak_hours(data):\n",
        "    data['hour'] = data['Date'].dt.hour\n",
        "    hourly_sales = data.groupby('hour')['total_price'].mean()\n",
        "    peak_hours = hourly_sales[hourly_sales > hourly_sales.mean() + hourly_sales.std()].index.tolist()\n",
        "    return peak_hours\n",
        "\n",
        "def is_peak_hour(date, peak_hours):\n",
        "    hour = date.hour\n",
        "    return hour in peak_hours\n",
        "\n",
        "def is_weekend(date):\n",
        "    return date.weekday() >= 5  # Saturday and Sunday\n",
        "\n",
        "def detect_anomalies_dynamic_threshold(data_points, reconstruction_errors, dates, window_size, peak_hours):\n",
        "    anomalies = []\n",
        "    consecutive_high_sales_count = 0\n",
        "\n",
        "    for i in range(window_size, len(data_points)):\n",
        "        window = data_points[i-window_size:i]\n",
        "        mean, std_dev = calculate_stats(window)\n",
        "\n",
        "        # Initial threshold\n",
        "        lower_threshold = mean - 3 * std_dev\n",
        "        upper_threshold = mean + 3 * std_dev\n",
        "\n",
        "        # Adjust threshold based on context\n",
        "        latest_point = data_points[i]\n",
        "        current_date = dates[i]\n",
        "        if is_weekend(current_date):\n",
        "            lower_threshold -= 0.5 * std_dev\n",
        "            upper_threshold += 0.5 * std_dev\n",
        "        if is_peak_hour(current_date, peak_hours):\n",
        "            lower_threshold -= 0.5 * std_dev\n",
        "            upper_threshold += 0.5 * std_dev\n",
        "\n",
        "        # Adjust threshold if the latest data point has been observed multiple times within the window\n",
        "        count_latest_point = window.count(latest_point)\n",
        "        if count_latest_point > 1:\n",
        "            lower_threshold -= 0.5 * std_dev\n",
        "            upper_threshold += 0.5 * std_dev\n",
        "\n",
        "        # Detect anomaly\n",
        "        if latest_point < lower_threshold or latest_point > upper_threshold:\n",
        "            anomalies.append((i, latest_point, reconstruction_errors[i]))\n",
        "        else:\n",
        "            # Check for continuous high sales\n",
        "            if latest_point > upper_threshold:\n",
        "                consecutive_high_sales_count += 1\n",
        "                if consecutive_high_sales_count >= 3:\n",
        "                    anomalies = [anomaly for anomaly in anomalies if anomaly[1] != latest_point]\n",
        "            else:\n",
        "                consecutive_high_sales_count = 0\n",
        "\n",
        "    return anomalies\n"
      ],
      "metadata": {
        "id": "AQR_bTgCuHRM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decompose_series(data, frequency):\n",
        "    try:\n",
        "        data.set_index('Date', inplace=True)\n",
        "        decomposition = seasonal_decompose(data['total_price'], model='additive', period=frequency)\n",
        "        trend = decomposition.trend\n",
        "        seasonal = decomposition.seasonal\n",
        "        residual = decomposition.resid\n",
        "\n",
        "        # Fill NaN values (could be improved depending on the use case)\n",
        "        trend = trend.fillna(method='bfill').fillna(method='ffill')\n",
        "        seasonal = seasonal.fillna(method='bfill').fillna(method='ffill')\n",
        "        residual = residual.fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "        return trend, seasonal, residual\n",
        "    except Exception as e:\n",
        "        print(f\"Error in seasonal decomposition: {e}\")\n",
        "        return None, None, None\n"
      ],
      "metadata": {
        "id": "3Fp1954Wx1Nh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decomposition(trend, seasonal, residual, title_prefix=''):\n",
        "    if trend is None or seasonal is None or residual is None:\n",
        "        print(\"Error: One or more components are None. Cannot plot.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(trend.index, trend, label='Trend')\n",
        "    plt.title(f'{title_prefix}Trend Component')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(seasonal.index, seasonal, label='Seasonal')\n",
        "    plt.title(f'{title_prefix}Seasonal Component')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(residual.index, residual, label='Residual')\n",
        "    plt.title(f'{title_prefix}Residual Component')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "pl-cOHmqwNOf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(train_filepath, test_filepath, autoencoder_model_path, output_filepath, week_file_path, frequency):\n",
        "    current_week = get_current_week()\n",
        "    last_retrained_week = get_last_retrained_week(week_file_path)\n",
        "\n",
        "    retrain = False\n",
        "    if last_retrained_week is None or current_week != last_retrained_week:\n",
        "        retrain = True\n",
        "        update_last_retrained_week(week_file_path, current_week)\n",
        "\n",
        "    train_data = read_and_preprocess_data(train_filepath, frequency)\n",
        "    test_data = read_and_preprocess_data(test_filepath, frequency)\n",
        "\n",
        "    # Perform seasonal decomposition\n",
        "    train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "    test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "    #trend, seasonal, residual = decompose_series(train_data, frequency)\n",
        "\n",
        "    # Plot the decomposed components\n",
        "    #plot_decomposition(trend, seasonal, residual, title_prefix='Train Data - ')\n",
        "\n",
        "    scaled_train_data, scaler, _ = scale_data(train_data)\n",
        "    scaled_test_data, _, _ = scale_data(test_data, scaler)\n",
        "\n",
        "    x_train = create_train_test_datasets(scaled_train_data)\n",
        "    x_test = create_train_test_datasets(scaled_test_data)\n",
        "\n",
        "    if retrain:\n",
        "        autoencoder = build_and_train_autoencoder(x_train, autoencoder_model_path, callbacks=[EarlyStopping(patience=10)])\n",
        "        print(\"Autoencoder model retrained.\")\n",
        "    else:\n",
        "        if os.path.exists(autoencoder_model_path):\n",
        "            autoencoder = load_model(autoencoder_model_path)\n",
        "            print(\"Loaded existing autoencoder model.\")\n",
        "        else:\n",
        "            autoencoder = build_and_train_autoencoder(x_train, autoencoder_model_path, callbacks=[EarlyStopping(patience=10)])\n",
        "            print(\"Autoencoder model trained and saved.\")\n",
        "\n",
        "    # Calculate reconstruction errors for anomaly detection\n",
        "    reconstruction_errors = calculate_reconstruction_errors(autoencoder, x_test)\n",
        "\n",
        "    # Detect anomalies with dynamic threshold\n",
        "    data_points = test_data['total_price'].tolist()\n",
        "    dates = pd.to_datetime(test_data['Date']).tolist()\n",
        "    window_size = 10  # Define your window size\n",
        "    peak_hours = calculate_peak_hours(train_data)\n",
        "    anomalies = detect_anomalies_dynamic_threshold(data_points, reconstruction_errors, dates, window_size, peak_hours)\n",
        "\n",
        "    # Print and save the results\n",
        "    test_dates = test_data['Date']\n",
        "    test_prices = test_data['total_price']\n",
        "\n",
        "    results = pd.DataFrame({\n",
        "        'Date': test_dates,\n",
        "        'Price': test_prices,\n",
        "        'Reconstruction Error': reconstruction_errors,\n",
        "        'Anomaly': [\"TRUE\" if i in [anomaly[0] for anomaly in anomalies] else \"FALSE\" for i in range(len(test_prices))]\n",
        "    })\n",
        "\n",
        "    results.to_csv(output_filepath, index=False)\n",
        "    print(results)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_filepath = '/content/drive/MyDrive/DatasetXTG/order_train_data.csv'\n",
        "    test_filepath = '/content/drive/MyDrive/DatasetXTG/order_test_data.csv'\n",
        "    autoencoder_model_path = 'autoencoder_model.h5'\n",
        "    output_filepath = 'anomalies_syntheticdata.csv'\n",
        "    week_file_path = 'last_retrained_week.txt'\n",
        "    frequency = 'W'  # Change to 'D' for daily, 'W' for weekly, etc.\n",
        "\n",
        "    results = main(train_filepath, test_filepath, autoencoder_model_path, output_filepath, week_file_path, frequency)\n",
        "    print(\"Anomalies detected and saved to:\", output_filepath)"
      ],
      "metadata": {
        "id": "1n1b9EGwuPRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f43edb1-6d88-4c1f-e710-6feff4a4e93f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.4818 - mae: 0.6793\n",
            "Epoch 2/50\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 0.4589 - mae: 0.6667"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4696 - mae: 0.6696 \n",
            "Epoch 3/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4623 - mae: 0.6594 \n",
            "Epoch 4/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4574 - mae: 0.6568\n",
            "Epoch 5/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.4607 - mae: 0.6589\n",
            "Epoch 6/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4499 - mae: 0.6517\n",
            "Epoch 7/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4561 - mae: 0.6597\n",
            "Epoch 8/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4546 - mae: 0.6583\n",
            "Epoch 9/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4451 - mae: 0.6506\n",
            "Epoch 10/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4413 - mae: 0.6437\n",
            "Epoch 11/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4382 - mae: 0.6428\n",
            "Epoch 12/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4391 - mae: 0.6447\n",
            "Epoch 13/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4245 - mae: 0.6335\n",
            "Epoch 14/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4308 - mae: 0.6410\n",
            "Epoch 15/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4164 - mae: 0.6280\n",
            "Epoch 16/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4187 - mae: 0.6326\n",
            "Epoch 17/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4111 - mae: 0.6228\n",
            "Epoch 18/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4034 - mae: 0.6170\n",
            "Epoch 19/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4045 - mae: 0.6201\n",
            "Epoch 20/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4071 - mae: 0.6216\n",
            "Epoch 21/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4090 - mae: 0.6252\n",
            "Epoch 22/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3923 - mae: 0.6115\n",
            "Epoch 23/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3867 - mae: 0.6042 \n",
            "Epoch 24/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3969 - mae: 0.6163\n",
            "Epoch 25/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3819 - mae: 0.6049 \n",
            "Epoch 26/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3778 - mae: 0.5980\n",
            "Epoch 27/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3694 - mae: 0.5934\n",
            "Epoch 28/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3682 - mae: 0.5888\n",
            "Epoch 29/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3616 - mae: 0.5828\n",
            "Epoch 30/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3579 - mae: 0.5796\n",
            "Epoch 31/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3603 - mae: 0.5839\n",
            "Epoch 32/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3412 - mae: 0.5663\n",
            "Epoch 33/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3376 - mae: 0.5622\n",
            "Epoch 34/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3357 - mae: 0.5629\n",
            "Epoch 35/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3328 - mae: 0.5596\n",
            "Epoch 36/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3242 - mae: 0.5513\n",
            "Epoch 37/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3183 - mae: 0.5487 \n",
            "Epoch 38/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3047 - mae: 0.5337\n",
            "Epoch 39/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3114 - mae: 0.5443 \n",
            "Epoch 40/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3028 - mae: 0.5347\n",
            "Epoch 41/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2959 - mae: 0.5294\n",
            "Epoch 42/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2817 - mae: 0.5149\n",
            "Epoch 43/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2760 - mae: 0.5083\n",
            "Epoch 44/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2829 - mae: 0.5168\n",
            "Epoch 45/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2578 - mae: 0.4886\n",
            "Epoch 46/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2547 - mae: 0.4871\n",
            "Epoch 47/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2551 - mae: 0.4900\n",
            "Epoch 48/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2456 - mae: 0.4801\n",
            "Epoch 49/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2322 - mae: 0.4629\n",
            "Epoch 50/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2240 - mae: 0.4574 \n",
            "Restoring model weights from the end of the best epoch: 50.\n",
            "Autoencoder model trained and saved.\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331ms/step\n",
            "         Date    Price  Reconstruction Error Anomaly\n",
            "0  2014-12-29  1995.60              0.011964   FALSE\n",
            "1  2015-01-05  4288.85              0.778837   FALSE\n",
            "2  2015-01-12  2918.55              0.317829   FALSE\n",
            "3  2015-01-19  3203.75              0.413779   FALSE\n",
            "4  2015-01-26  3535.80              0.525490   FALSE\n",
            "5  2015-02-02  2926.30              0.320436   FALSE\n",
            "6  2015-02-09  3351.50              0.463486   FALSE\n",
            "7  2015-02-16  3325.05              0.454587   FALSE\n",
            "8  2015-02-23  3112.05              0.382928   FALSE\n",
            "9  2015-03-02  3270.65              0.436286   FALSE\n",
            "10 2015-03-09  3226.50              0.421432   FALSE\n",
            "11 2015-03-16  3057.55              0.364593   FALSE\n",
            "12 2015-03-23  2620.30              0.217489    TRUE\n",
            "13 2015-03-30  2969.70              0.335037   FALSE\n",
            "14 2015-04-06  3424.55              0.488062   FALSE\n",
            "15 2015-04-13  3580.20              0.540427   FALSE\n",
            "16 2015-04-20  2972.70              0.336047   FALSE\n",
            "17 2015-04-27  2942.55              0.325903   FALSE\n",
            "18 2015-05-04  3103.30              0.379984   FALSE\n",
            "19 2015-05-11  3385.40              0.474891   FALSE\n",
            "20 2015-05-18  3140.80              0.392600   FALSE\n",
            "21 2015-05-25  3291.45              0.443283   FALSE\n",
            "22 2015-06-01  3675.95              0.572640   FALSE\n",
            "23 2015-06-08  2984.80              0.340118   FALSE\n",
            "24 2015-06-15  3058.45              0.364896   FALSE\n",
            "25 2015-06-22  3170.60              0.402626   FALSE\n",
            "26 2015-06-29  3907.55              0.650557    TRUE\n",
            "27 2015-07-06  3146.55              0.394535   FALSE\n",
            "28 2015-07-13  3131.55              0.389488   FALSE\n",
            "29 2015-07-20  3388.30              0.475866   FALSE\n",
            "30 2015-07-27  2687.70              0.240165   FALSE\n",
            "31 2015-08-03  3053.35              0.363180   FALSE\n",
            "32 2015-08-10  3156.95              0.398034   FALSE\n",
            "33 2015-08-17  3747.80              0.596812   FALSE\n",
            "34 2015-08-24  2979.90              0.338469   FALSE\n",
            "35 2015-08-31  2941.75              0.325634   FALSE\n",
            "36 2015-09-07  3398.40              0.479264   FALSE\n",
            "37 2015-09-14  3023.15              0.353020   FALSE\n",
            "38 2015-09-21  2083.55              0.036962    TRUE\n",
            "39 2015-09-28  3447.30              0.495716   FALSE\n",
            "40 2015-10-05  2757.50              0.263647   FALSE\n",
            "41 2015-10-12  2722.80              0.251973   FALSE\n",
            "42 2015-10-19  2756.25              0.263227   FALSE\n",
            "43 2015-10-26  2659.50              0.230677   FALSE\n",
            "44 2015-11-02  3310.15              0.449575   FALSE\n",
            "45 2015-11-09  2927.00              0.320672   FALSE\n",
            "46 2015-11-16  3198.85              0.412130   FALSE\n",
            "47 2015-11-23  4262.70              0.770039    TRUE\n",
            "48 2015-11-30  3303.35              0.447287   FALSE\n",
            "49 2015-12-07  3113.55              0.383433   FALSE\n",
            "50 2015-12-14  3116.20              0.384324   FALSE\n",
            "51 2015-12-21  2292.40              0.107174   FALSE\n",
            "52 2015-12-28  1316.45              0.221163    TRUE\n",
            "Anomalies detected and saved to: anomalies_syntheticdata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_S9HmuDwNU8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}